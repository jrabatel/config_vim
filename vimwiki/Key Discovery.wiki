= TODO =

== HCA ==
* [X] Implementing the statistics-based pruning
  * [X] what about the support measure?
* [X] Adding the discriminability measure
  * [X] computation for FAK
* [ ] Functional dependencies with FAK
  * [ ] several possibilities
    * [ ] try to take into consideration the instances that are not covered by a FD, but it is difficult (impossible?) to agregate the measures (support, distinct value count) without rescanning everything or storing additional information. 
    * [ ] retrieve different types of FD of the form X->A, considering whether they will help generating uniques or non-uniques (c is the initial AttributeSet and c' the one obtained via the application of the FD)
      * [ ] if supp(X) = supp(X,A)
        * [ ] replacing X by A: if c was non-unique, c' is non-unique as well
        * [ ] replacing A by X: if c was unique, c' is unique as well
* [ ] Adding the SK
  * [ ] Functional dependencies
  * [ ] Discriminability
  * [ ] Using the distinct values and histograms
* [ ] Adding a mechanism for writing the data on disk
* [ ] if possible, consider the discriminability measure with functional dependency (i.e., a FD holds for a certain discriminability).
* [ ] implement a sampling mechanism, in order to call Gordian and prune some non-uniques

== Gordian ==
* [ ] Implementing the FAK principles
* [ ] Adding the discriminability measure
* [ ] Adding the SK

== General ==
* [X] Adding the connection to Jena
* [X] Loading the data from Jena
  * [X] via a datafile
  * [X] via an indexed database
* [X] Installing Jérôme David's code
* [X] Integrate the Abes dataset and test it
* [ ] How to quickly compute the support of an AttributeSet ?
  * [ ] represent each instance with a bitmap corresponding to covered attributes?
    * [ ] then, computing the support simply will not require to scan all the instances:
      * [ ] create the bitmap corresponding to an AttributeSet
      * [ ] for each instance, check whether the AttributeSet is included
* [ ] Profiling time consumption
  * [ ] candidate generation
  * [ ] candidate checking
  * [ ] FD retrieval
  * [ ] FD checking
  * [ ] sorting the lists of instances to check
* [X] Speed-up
  * [X] When mining FAK, we should store the data in main memory directly in single-valued version, it would reduce time consumption
* [X] Memory efficiency
  * [X] consider one class at a time, and then remove all the instances, attributes, and values related to the past class

= KD2R =

== Basic definitions ==
S --> data source
C --> class

*Non keys* : 
A set of property expressions is a non-key for the class C in S if: 
There exist X, Y in C such that Y and X are different but share the same values on the property expressions

*Keys* :
For all X and Y in C, there exists at least one property expression such that the values of X and Y differ

*Undetermined Keys* :
A set of property expressions is an undetermined key if
 - it is not a non-key
 - for all property expression X and Y have the same value
 - there exists one property expression such that there does not exist any value for X and Y

Top-Down algorithm: the keys discovered for a given class are derived from its subclasses (i.e., more specific classes).

== Prefix Tree ==
 - each level corresponds to one property expression (i.e., one property)
 - each cell at a given level contains:
   - a value for the corresponding level
   - an (boolean) attribute to specifiy whether the value is null or not
   - a list of URI to the corresponding class instances
 - a prefix-path represents the set of instances that share the same values for the properties involvedin the path

=== Intermediate Prefix Tree ===
 - to integrate the non-existence of some property values (i.e., null values), an intermediate prefix tree is built
 - the principle is simply to build null values and integrate them to the prefix tree
 - then, the final prefix tree is generated by replacing those newly create null values by the set of all possible values (i.e., the ones observed in the data)

== UNK-Finder Algorithm ==
Given the root of the prefix tree for a class, the number of attributes, and the already known keys (for subclasses)

3 types of pruning:
 - subsumption relation (by considering a set of inherited keys)
 - anti-monotonicity of non and undetermined keys
 - the monotonic property of keys

== Global remarks ==
 - Is it the complete set of keys which is retrieved? Indeed, the construction of the prefix-tree seems to impose which are the keys that will be checked... --> I do not understand something...


= FD_MINE (DMKD 2008) =
 - Mine FD in data

== Related Work ==
 - 3 categories of existing work:
   - candidate generate-and-test approach
   - minimal cover approach
   - formal concept analysis approach


= Advancing the discoveyr of Unique column combinations (CIKM 2011) =
Principle:
 - Gordian uses a row-based approach
 - There exist also some column-based approaches (based on Apriori)
 - Focus is on the candidate generation strategy because checking whether a candidate is unique is costly. => the set of generated candidates must be as small as possible

Statistics-based pruning
 - Principle: find some functional dependencies on the fly and exploit them
 - for a combination of columns, it retrieves:
   - the number of distinct values and the histogram of value frequencies (build the histogram is in O(n.log(n)) ), or
   - only the number of distinct values
 - candidate is unique if it contains as many distinct values as there are tuples in the table or if all value frequencies are 1

IDEAS
 - is it possible to use a pattrn-growth approach instead of a apriori approach to generate the candidates?
 - is it possible to exploit the prefix-tree structure of Gordian?
 - is there other pruning rules that can be integrated?


= Scharffe =
RDF data
a predicate can be multivaluated
the partition of a set of predicates

_partition for a predicate_: the set of sets of subjects such that the elements of each set have the same image through p
_partition for a set of predicates_: the set of sets of 
